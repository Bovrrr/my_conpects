

## способы передачи информации по комп. сети
 - ethernet
 - 
 - angara (российская, маленький latency)

## языки для лучшей утилизации выч. ресурсов

**GPU**
 - CUDA (nvidia)
 - OpenCL (открытый. на нём сложно писать)
 - HIP (amd)
 - Open Acc

**CPU**
 - OpenMP (позволяет использовать pthreads через директив препроцессора компилятора)
 - Open Acc
 - pthreads (довольно низкоуровниевая)


## как и где лучше использовать omp/mpi

> omp позволяет создавать дополнительные потоки
> mpi позволяет создовать новые процессы

 - создавать новые процессы дороже по ресурсам, поэтому внутри узлов используем omp (потоки), между узлами используем mpi (процессы)
 - на gpu узлах используем cuda/hip/и тд


## классификация Флинна
(было на 1-ом занятии)

- avx - штука в cpu, позволяющая делать некоторые арифметические операции векторно


## системы с распределённой памятью

 - \*почитать про разные оптимальные топологии компьютерных сетей\*

**термины**
 - *bandwidth* (пропускная способность) - сколько почты может осилить почты за единицу времени
 - *latency* - как быстро дойдёт сообщение (задержка)
 - *синхронное/асинхронное* сообщение - нужно ли ждать ответа от адресата сообщения



## mpi lib (message passing interface)

 - ресурсы и память не общие для процессов. коммуникация только за счёт передачи сообщений
 - MPI_COMM_SELF - указатель на себя
 -  коммуникатор - объединение процессов и коммуникации между ними (инкапсуляция коммуникаций)
 - MPI_COMM_WORLD - коммуникатор всех процессов
 - у каждого процесса есть свой уникальный номер и его можно узнать



## небольшой пример кода с MPI

![[Pasted image 20221031175408.png]]


## коммуникаця point2point

- блокирующие send/recieve (если получатель не примет сообщение, программа не завершится)


отправка
```
MPI_Send(
void* data, // указатель на данные/пакет
int count, // размер пакета
MPI_Datatype, // тип значений в пересылаемом пакете
int destination, // rank (номер) процесса получаетеля
int tag, // tag сообщения
MPI_Comm communicator // внутри какого коммуникатора будет производится пересылка
)
```



получение
```
MPI_Recv(
void* data, // указатель на данные/пакет
int count, // размер пакета
MPI_Datatype, // тип значений в пересылаемом пакете
int source, // rank (номер) процесса отправителя
int tag, // tag сообщения
MPI_Comm communicator, // внутри какого коммуникатора будет производится пересылка
MPI_Status* status // удобно для дебага
)
```


![[Pasted image 20221031180223.png]]

 - безопасный синхронизированный send (сначала устанавливает контакт - готов ли получатель получить сообщение)
![[Pasted image 20221031180731.png]]

- отправка через буффер
![[Pasted image 20221031180846.png]]

- самый небезопасный send (можно потерять данные)
![[Pasted image 20221031181047.png]]

- обычный send
	- если данные маленькие, то через буффер (в буффер на получателе)
	- если данных много, то SSend
![[Pasted image 20221031181142.png]]


- **Таблица сравнения**
![[Pasted image 20221031181403.png]]



- неблокирующие отправки (немного напутано с tag)
![[Pasted image 20221031181749.png]]


## mpi collective communication

**Reduce**
 - создаём O(log2 n) процессов и всё готово
![[Pasted image 20221101140028.png]]
![[Pasted image 20221101140352.png]]
**AllReduce**
 - результат получают все процессы
![[Pasted image 20221101140408.png]]

**BroadCast**
![[Pasted image 20221101140608.png]]
**Scatter**
 - раздаём значения вектора по процессам (разливаем данные по процессам)
![[Pasted image 20221101140624.png]]
**Gather**
 - со всех процессов собираем данные на 1 процесс
![[Pasted image 20221101140648.png]]

**AllGather**
![[Pasted image 20221101140748.png]]


## замер времени в mpi
![[Pasted image 20221101140901.png]]


## семинар (код часть)

 - запустить программу
```
mpirun -n process_num program_name
```
мб добавить 
`--allow-run-as-root`

> немного про гугл колаб
> % - магия линии
> \%% - магия ячейки

 - скомпилировать с/с++ программу
```
mpicc file_name.c -o file_name.o
```



## вычисления на

 - нужно понять как разбивать сетки на части на разные узлы/ядра
 - как обёртывать их в призрочные слои, которые потом будет удобно пересылать/получать между процессами


**нужно**
 - оверхед по памяти из-за фиктивных слоёв для коммуникации были сильно дешевле впихивания целой сетки в 1 узел
 - чтобы затраты на вычисления были дороже, чем затраты на коммуникацию








