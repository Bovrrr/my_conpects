

# off policy алгоритмы с континуальным пространством действий



вспоминаем прошлый материал
## **deep Q network**
 - позволяет не знать истинную динамику среды
 - позволяет работать с континуальным пространством состояний
![[Pasted image 20221124220214.png]]
Приближаем Q-функцию с помощью нейронной сети (минимизируем MSE, используем target DQN как учителя)


## боремся с дискретным пространством действий с помощью on-policy алгоритмов

![[Pasted image 20221124220510.png]]

Минусы on policy алгоритмов
 - нестабильность при обучении
 - сложность подбора гиперпараметров
 - всегда требуют свежые данные



## DQN + Actor-Critic

![[Pasted image 20221124221055.png]]
 - в континуальном пр-ве действий мы меняем добавляем шум как случайность при принятии выбора действия (считаем, что близкие в пр-ве действий действия приводят к похожим результатам)
 - есть нейронка для приближения Q-функции и есть ещё одна нейронка для приближения максимума Q-функции (звучит как просто V-функция). + по таргет сети для каждой
 - меняем таргет сеть не каждые N батчей, а через *скользящее среднее* (tau ~ 1е-2, 1е-3). Чем дольше мы можем позволить себе обучение, чем *меньше мы возьмём tau, тем стабильнее будет обучение*. называется *soft-update*
 - можно заменить MSE на L1 smooth loss. L1 лосс лучше и стабильнее работает с большими значениями
 - тут мы используем буффер опыта, а не копим фулл траектории, поэтому мы можем использовать улучшения, которые мы использовали при работе с DQN  (приоритизированный опыт)
 
![[Pasted image 20221124221956.png]]

**Основная проблема DDPG** - завышение предсказываемых значений награды.
Это приводит к субоптимальным политикам или вовсе неверным решениям.

![[Pasted image 20221124223810.png]]
- закладывая ошибку в таргет сеть, мы будем накапливать ошибку завышения предсказываемой награды


![[Pasted image 20221124224048.png]]
- в какие-то моменты времени, когда ошибка положительна, выполняются первые 3 неравенства. из них следует 4-ое - завышение награды

**решение проблемы завышения предсказываемой награды**

![[Pasted image 20221124224613.png]]
- 2 критика
	 - используем 2-х критиков для оценки вместо 1-го
	 - в лосс ф-ции берём минимум из их предсказаний, а политику обновляем всегда только по одному (1-му или 2-му. нужно просто выбрать заранее)
	 - критиков можем обновлять на разных батчах для меньшей корреляции
- добавляем шума при выборе действия
![[Pasted image 20221124230710.png]]


Добавим **стохастики в политику**
 - добавим ентропию в политику
![[Pasted image 20221124231159.png]]
![[Pasted image 20221124231510.png]]

TD3 со стохастической политикой
![[Pasted image 20221124231720.png]]
![[Pasted image 20221124231919.png]]

1:04:00 - конец 1-ой части лекции




















































































































